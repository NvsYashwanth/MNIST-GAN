# MNIST-GAN

## ***The Discriminator architecture***

* The discriminator is going to be a typical linear classifier.
* The activation function we will be using is Leaky ReLu.

### ***Why leaky ReLu?***
* We should use a leaky ReLU to allow gradients to flow backward through the layer unhindered. A leaky ReLU is like a normal ReLU, except that there is a small non-zero output for negative input values.


## ***The Generator architecture***
* The generator uses latent samples to make fake images. These latent samples are vectors which are mapped to the fake images.
* The activation function for all the layers remains the same except we will be using Tanh at the output.

### ***Why Tanh at the output?***
* The generator has been found to perform the best with ğ‘¡ğ‘ğ‘›â„tanh for the generator output, which scales the output to be between -1 and 1, instead of 0 and 1.Â 

## ***Scaling images***
* We output of the generator to be comparable to the real images pixel values, which are normalized values between 0 and 1. Thus, we'll also have to scale our real input images to have pixel values between -1 and 1 when we train the discriminator.Â 
This will be done during the training phase.

## ***Generalization***
* To help the discriminator generalize better, the labels are reduced a bit from 1.0 to 0.9. For this, we'll use the parameter smooth; if True, then we should smooth our labels. In PyTorch, this looks like:
```labels = torch.ones(size) * 0.9```
* We also made use of dropout layers to avoid overfitting.

## ***Loss calculation***
* The discriminator's goal is to output a 1 for real and 0 for fake images. On the other hand, the generator wants to make fake images that closely resemble the real ones.
* Thus we can say if "D" represents the loss for the discriminator, then the following can be stated:

<p align='center'>
The goal of discriminatorÂ : D(real_images)=1 & D(fake_images)=0
</p>

<p align='center'>
The goal of generator: D(real_images)=0 & D(fake_images)=1
</p>

* We will use BCEWithLogitsLoss, which combines a sigmoid activation function (we want the discriminator to output a value 0â€“1 indicating whether an image is real or fake) and binary cross-entropy loss.
Equation of Binar Cross-Entropy Loss.

---

TrainingÂ 
As mentioned earlier, Adam is a suitable optimizer.
The generator takes in a vector z and outputs fake images. The discriminator alternates between training on the real images and that of the fakes images produced by the generator.
Steps involved in discriminator training:
We first compute the loss on real images
Generate fake images
Compute loss on fake images
Add the loss of the real and fake images
Perform backpropagation and update weights of the discriminator

Steps involved in generator training:
Generate fake images
Compute loss on fake images with inversed labels
Perform backpropagation and update the weights of the generator.




Code by the author.Training loss
We shall plot generator and discriminator losses against the number of epochs.



Code by the author.Training losses. Image by theÂ author.

Samples generated by the generator
At the start



Code by the author.Image by theÂ author.Overtime



Code by the author.Image by theÂ author.This way the generator starts out with noisy images and learns over time.


---

Conclusions
Since the time Ian Goodfellow and his colleagues at the University of Montreal designed GANs, they exploded with popularity. The number of applications is remarkable. GANs were further improved by many variations some of which are CycleGAN, Conditional GAN, Progressive GAN, etc. To read more about these check out this link. Now open a Jupyter notebook and try to implement whatever you learned.
Thank you. See you at the nextÂ one.
